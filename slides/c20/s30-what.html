<h1>
  Supervised Machine Learning - 3 </h1>
  $$ h(x) = \theta_0 + \theta_1 x $$

  <p>
    where θ<sub>0</sub> and θ<sub>1</sub> are constants. Our goal is to find the perfect values of θ<sub>0</sub> and θ<sub>1</sub> to make our predictor work as well as possible.
  </p>

  <p>
  Optimizing the predictor h(x) is done using training examples. For each training example, we have an input value x_train,
   for which a corresponding output, y, is known in advance. For each example,
   we find the difference between the known, correct value y, and our predicted value h(x_train).
    With enough training examples, these differences give us a useful way to measure the “wrongness” of h(x).
    We can then tweak h(x) by tweaking the values of θ<sub>0</sub> and θ<sub>1</sub> to make it “less wrong”.
    This process is repeated over and over until the system has converged on the best values for θ<sub>0</sub> and θ<sub>1</sub>.
    In this way, the predictor becomes trained, and is ready to do some real-world predicting.

</p>
